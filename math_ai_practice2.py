# -*- coding: utf-8 -*-
"""math-ai-practice2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bFyc-bxDfGc61Aw2xafj1A71zcPIBtFe
"""

import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""## Datasets

"""

train = pd.read_csv('/kaggle/input/math-qsa-dataset/train.csv')
test = pd.read_csv('/kaggle/input/math-qsa-dataset/test.csv')

train.head()

test.head()

"""## Models"""

from tqdm.notebook import tqdm
tqdm.pandas() # progress bar for pandas

import keras
import keras_nlp

import plotly.graph_objs as go
import plotly.express as px
from IPython.display import display, Markdown

from transformers import GemmaModel, GemmaConfig, GemmaForCausalLM, FlaxGemmaForCausalLM, AutoTokenizer, AutoModelForCausalLM

pip install -U transformers datasets accelerate peft trl bitsandbytes wandb

import gc
import os

import torch
import wandb
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
)
from trl import ORPOConfig, ORPOTrainer, setup_chat_format

import plotly.graph_objs as go
import plotly.express as px
from IPython.display import display, Markdown

from tqdm.notebook import tqdm
tqdm.pandas()

# torch_dtype = torch.float16
# attn_implementation = "eager"

base_model = "nvidia/OpenMath-CodeLlama-7b-Python-hf"

# # QLoRA config
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch_dtype,
#     bnb_4bit_use_double_quant=True,
# )

# # LoRA config
# peft_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM",
#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']
# )

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float32
)

tokenizer = AutoTokenizer.from_pretrained("nvidia/OpenMath-CodeLlama-7b-Python-hf")

"""## making template"""

template = """Role:\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n\nInstruction:
1. Carefully read and comprehend the problem statement provided in the "Problem" section.
2. In the "Solution" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.
3. At the end, create a "Answer" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n\nProblem:\n{problem}\n\nSolution:\n{solution}"""

def is_integer(text):
    try:
        if int(text) >= 0:
            return True
        else:
            return False
    except ValueError:
        return False

train["is_integer"] = train.answer.map(is_integer)
train = train[train.is_integer].reset_index(drop=True)
train.head(2)

from tqdm.notebook import tqdm
tqdm.pandas()

train["prompt"] = train.progress_apply(lambda row: template.format(problem=row.problem,
                                                             solution=f"{row.solution}\n\nAnswer:\n{row.answer}"),
                                                             axis=1)
data = train.prompt.tolist()

def colorize_text(text):
    for word, color in zip(["Role", "Instruction", "Problem", "Solution", "Answer"],
                           ["blue", "yellow", "red", "cyan", "green"]):
        text = text.replace(f"{word}:", f"\n\n**<font color='{color}'>{word}:</font>**")
    return text

sample = data[12]

# Give colors to Instruction, Response and Category
sample = colorize_text(sample)

# Show sample in markdown
display(Markdown(sample))

train.head()

row = train.iloc[12]

# Generate Prompt using template
prompt = template.format(
    problem=tokenizer(row["problem"], return_tensors="pt"),
    solution="",
)

"""## Trial 2"""

template = """Role:\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n\nInstruction:
1. Carefully read and comprehend the problem statement provided in the "Problem" section.
2. In the "Solution" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.
3. At the end, create a "Answer" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n\nProblem:\n{problem}\n\nSolution:\n{solution}"""

problem_data = train.iloc[12]
problem = train['problem']
solution = train.get('solution', '')

# 템플릿을 이용해 프롬프트 생성
input_text = template.format(problem=problem, solution=solution)

# 입력을 토큰화
input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)

# 모델로부터 답 생성
outputs = model.generate(
    input_ids,
    max_new_tokens=512,
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True,
    temperature=0.6,
    top_p=0.9
)

# 생성된 답을 디코딩
# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# 결과 출력
print(f"**생성된 답변:**\n{generated_text}\n")

def colorize_text(text):
    for word, color in zip(["Role", "Instruction", "Problem", "Solution", "Answer"],
                           ["blue", "yellow", "red", "cyan", "green"]):
        text = text.replace(f"{word}:", f"\n\n**<font color='{color}'>{word}:</font>**")
    return text

# Colorize
output = colorize_text(generated_text)

# Display in markdown
display(Markdown(generated_text))